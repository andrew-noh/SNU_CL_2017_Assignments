<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <meta name="GENERATOR" content="Namo WebEditor">
    <title>Assignment 4: </title>
    <meta http-equiv="content-type" content="text/html; charset=EUC-KR">
  </head>
  <body>
    <p align="center"><b><font face="Trebuchet MS" size="5">Assignment
          4: &nbsp;Simple Neutral Network을 이용한 IMDB movie review 데이터
          감정분석</font></b><font face="Trebuchet MS"></font></p>
    <p><font face="Trebuchet MS" size="4">목표: 지금까지 배운 Simple Forward
        Neural Network을 영화평 리뷰의 감정분석(postive, negative)에 활용할 수 있도록 모델 구축</font><font
        face="Trebuchet MS"></font></p>
    <p><font face="Trebuchet MS" size="4">개요: &nbsp;</font><font
        face="Trebuchet MS"></font></p>
    <ul>
      <li><font face="Trebuchet MS" size="4">Keras를 활용한 </font><a
href="https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/"><font
            face="Trebuchet MS" size="4">How to Develop Word Embedding
            Model for Predicting Movie Review Sentiment</font></a><font
          face="Trebuchet MS" size="4"> 사이트를 활용하여 keras 대신 simple neural
          network을 이용하여 movie review의 감정을 분석하는 모델을 개발</font><font
          face="Trebuchet MS"></font></li>
      <li><font face="Trebuchet MS" size="4">대부분의 코드는 </font><a
href="https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/"><font
            face="Trebuchet MS" size="4">How to Develop Word Embedding
            Model for Predicting Movie Review Sentiment </font></a><font
          face="Trebuchet MS" size="4">사이트에서 제공하는 것을 그대로 사용할 수 있으나
          &nbsp;모델을 구축하고 이를 위한 텍스트를 integer로 변환하는 texts_to_sequence, 와
          문서의 길이를 통일하기 위한 pad_sequences는 직접 구현 </font><font
          face="Trebuchet MS"></font></li>
      <li><font face="Trebuchet MS" size="4">영화평은 각각 1,000개의
          긍정(positive), 부정(negative) 문서로 되어 있고 각각 900씩 총 1, 800문장을
          training set으로 나머지 각 100 문장을 test set으로 사용한다. </font><font
          face="Trebuchet MS"></font></li>
      <li><font face="Trebuchet MS" size="4">기본 방법은 긍정문서에 나타나는 단어와 부정
          문서에 나타나는 단어를 학습하여 문서분류를 행하는 것으로 어떤 word embedding도 사용하지 않는다.</font><font
          face="Trebuchet MS"></font></li>
      <li><font face="Trebuchet MS" size="4">문서를 training, test set으로
          분리하는 코드에서 요구되는 25,767 여개의 어휘를 추출하는 해당 사이트의 2.Data Preparation에
          있으나 이를 이미 정리해놓은 </font><a href="vocab.txt"><font
            face="Trebuchet MS" size="4">vocab.tx</font></a><font
          face="Trebuchet MS" size="4">t는 여기서 제공. 따라서 3.Train Embedding
          Layer부터 참조할 수 있음</font></li>
      <li><font face="Trebuchet MS" size="4">class TwoLayerNet과 관련된 모든
          클래스와 함수를 포함할 것. </font><font face="Trebuchet MS"></font></li>
      <li><font face="Trebuchet MS" size="4">나중에는 MutliLayerNet을 활용하여
          성능을 더 개선시킬 것임</font></li>
      <li><font face="Trebuchet MS" size="4">Assignment4_yourhakbun.py<br>
        </font></li>
    </ul>
    <p><font face="Trebuchet MS">&nbsp;</font></p>
    <p><b><font face="Trebuchet MS">def load_doc(filename):</font></b><font
        face="Trebuchet MS"></font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;그대로 사용</font></p>
    <p><font face="Trebuchet MS"># turn a doc into clean tokens</font></p>
    <p><b><font face="Trebuchet MS">def clean_doc(doc, vocab):</font></b><font
        face="Trebuchet MS"></font></p>
    <p><font face="Trebuchet MS">그대로 사용</font></p>
    <p><b><font face="Trebuchet MS">def process_docs(directory, vocab,
          is_trian):</font></b></p>
    <p><font face="Trebuchet MS">그대로 사용</font></p>
    <p><font face="Trebuchet MS">#encoding</font></p>
    <p><b><font face="Trebuchet MS">def texts_to_sequence(train_docs):</font></b></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;텍스트를 integer로 바꾸어 모델에
        입력. 정규화 할 경우 전체 단어 수로 각 정수 값을 나누면 0-1사의 값으로 normalize됨</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;입력 train_docs는 긍정, 부정의 영화평을
        읽어 각 문서를 한 리스트에 저장된 list of list 구조</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;우선 여기에 사용된 모든 어휘를
        &nbsp;정수로 바꾸기 위한 중복된 어휘를 제거한 목록을 얻기 위한 flat_words&nbsp;&nbsp;</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;이 어휘 각각을 정수로
        전환. enumerate를 사용하여 어휘의 리스트와 그 인덱스를 딕셔너리에 key와 value로 하여 바꿈</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;#encode
        char-&gt;int,</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;char_to_int =
        dict((c, i) for i, c in enumerate(flat_words))</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;이제 train_docs 리스트에서 각
        문서 리스트를 하나씩 불러와 개별 어휘를 정수로 바꾸고 다시 normalize하여 그 값을 계속 저장.</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;최종 리턴되는 구조는
        &nbsp;각 어휘가 &nbsp;0-1사이의 값으로 바뀐 입력 train_docs와 마찬가지인 list of
        list 구조를 리턴</font></p>
    <p><font face="Trebuchet MS">&nbsp;</font></p>
    <p><font face="Trebuchet MS">#padding</font></p>
    <p><b><font face="Trebuchet MS">def pad_sequences(sequences, number,
          width):</font></b></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;simple neural network을 구성하기
        위해 각 문서별 어휘의 수가 동일해야 균일한 입력 행렬을 구축할 수 있기 때문에 문서별로 다른 어휘 수를 최대
        어휘수에 맞추기 위해 비어 있는 부분은 0으로 채우는 padding 구현</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;입력은 text_toz_sequence에서 리턴된
        list of list 구조인 sequences와 채워 넣을 수 인 number( 이 경우는 0), 그리고 최대
        어휘 길이를 나타내는 width를 받아서 각 문서에서 최대 길이에 맞춰 0을 채워 넣는 모듈 작성</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;여러 방법이 있으나 list.extend를
        사용하고 itertools의 repeat를 사용하면 간단.</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;반환되는 값은 0으로 채워져 각 문서가
        동일한 list of list 구조</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;return
        padd_seq_out</font></p>
    <p><font face="Trebuchet MS"><b>def one_hot_encoding(x):</b></font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;이전 과제에서 사용했던 것
        그대로 사용</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;return output</font></p>
    <p><font face="Trebuchet MS">&nbsp;</font></p>
    <p><font face="Trebuchet MS"># load the vocabulary</font></p>
    <p><font face="Trebuchet MS">vocab_filename = 'vocab.txt'</font></p>
    <p><font face="Trebuchet MS">vocab = load_doc(vocab_filename)</font></p>
    <p><font face="Trebuchet MS">vocab = vocab.split()</font></p>
    <p><font face="Trebuchet MS">vocab = set(vocab)</font></p>
    <p><font face="Trebuchet MS">&nbsp;</font></p>
    <p><font face="Trebuchet MS"># load all training reviews</font></p>
    <p><font face="Trebuchet MS">positive_docs =
        process_docs('txt_sentoken/pos', vocab, True)</font></p>
    <p><font face="Trebuchet MS">negative_docs =
        process_docs('txt_sentoken/neg', vocab, True)</font></p>
    <p><font face="Trebuchet MS">train_docs = negative_docs +
        positive_docs</font></p>
    <p><font face="Trebuchet MS">encoded_docs =
        texts_to_sequence(train_docs)</font></p>
    <p><font face="Trebuchet MS" color="red">#padding을 위해 최대 길이의 문서 길이를
        구함</font></p>
    <p><b><font face="Trebuchet MS" color="red">max_length = max([len(s)
          for s in encoded_docs])</font></b><font face="Trebuchet MS"></font></p>
    <p><font face="Trebuchet MS">&nbsp;</font></p>
    <p><font face="Trebuchet MS" color="red">x_train =
        pad_sequences(encoded_docs, 0, max_length)</font></p>
    <p><font face="Trebuchet MS" color="red">#x_train을 모델 학습을 위한 numpy
        array로 변환할 필요</font></p>
    <p><font face="Trebuchet MS" color="red">x_train = </font></p>
    <p><font face="Trebuchet MS"># define training labels</font></p>
    <p><font face="Trebuchet MS">t_train = array([0 for _ in range(900)]
        + [1 for _ in range(900)])</font></p>
    <p><font face="Trebuchet MS">t_train = one_hot_encoding(t_train)</font></p>
    <p><font face="Trebuchet MS" color="red"># test 데이터도 마찬가지로 준비</font><font
        color="red"></font></p>
    <p># load all test reviews</p>
    <p>positive_docs = process_docs('txt_sentoken/pos', vocab, False)</p>
    <p>negative_docs = process_docs('txt_sentoken/neg', vocab, False)</p>
    <p>test_docs = negative_docs + positive_docs</p>
    <p><font color="red">encoded_docs = texts_to_sequence(test_docs)</font></p>
    <p>&nbsp;</p>
    <p><font color="red"># 이 테스트 데이터에서도 max_length를 구해 padding에 사용해야
        하나... 주의할 점은 training data의 max_length와 test data의 max_length가
        서로 다를 경우</font></p>
    <p><font color="red">모델의 입력 형상이 달라지기 때문에 둘 중에서 더 큰 것으로 통일</font></p>
    <p>max_length = max([len(s) for s in encoded_docs])</p>
    <p>&nbsp;</p>
    <p><font color="red">x_test = pad_sequences(encoded_docs, 0,
        max_length)</font></p>
    <p>x_test =</p>
    <p>&nbsp;</p>
    <p># define training labels</p>
    <p>t_test = array([0 for _ in range(100)] + [1 for _ in range(100)])</p>
    <p>#ytrain = np.array(ytrain, dtype=int)</p>
    <p>t_test = one_hot_encoding(t_test)</p>
    <p><font face="Trebuchet MS">class TwoLayerNet:</font></p>
    <p><font face="Trebuchet MS">....</font></p>
    <p><font face="Trebuchet MS" color="red">network =
        TwoLayerNet(input_size=, hidden_size=50, output_size=)</font></p>
    <p><font face="Trebuchet MS"># 하이퍼파라미터</font></p>
    <p><font face="Trebuchet MS">iters_num = 10000 &nbsp;# 반복 횟수를 적절히
        설정한다.</font></p>
    <p><font face="Trebuchet MS">train_size = x_train.shape[0]</font></p>
    <p><font face="Trebuchet MS">batch_size = 100 &nbsp;&nbsp;# 미니배치 크기</font></p>
    <p><font face="Trebuchet MS">learning_rate = 0.1</font></p>
    <p><font face="Trebuchet MS">&nbsp;</font></p>
    <p><font face="Trebuchet MS">train_loss_list = []</font></p>
    <p><font face="Trebuchet MS">train_acc_list = []</font></p>
    <p><font face="Trebuchet MS">test_acc_list = []</font></p>
    <p><font face="Trebuchet MS">&nbsp;</font></p>
    <p><font face="Trebuchet MS"># 1에폭당 반복 수</font></p>
    <p><font face="Trebuchet MS">iter_per_epoch = max(train_size /
        batch_size, 1)</font></p>
    <p><font face="Trebuchet MS">&nbsp;</font></p>
    <p><font face="Trebuchet MS">for i in range(iters_num):</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;# 미니배치 획득</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;batch_mask =
        np.random.choice(train_size, batch_size)</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;x_batch =
        x_train[batch_mask]</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;t_batch =
        t_train[batch_mask]</font></p>
    <p><font face="Trebuchet MS">&nbsp;</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;#print("BATCH",
        x_batch.shape, t_batch.shape)</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;# 기울기 계산</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;#grad =
        network.numerical_gradient(x_batch, t_batch)</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;grad =
        network.gradient(x_batch, t_batch)</font></p>
    <p><font face="Trebuchet MS">&nbsp;</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;# 매개변수 갱신</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;for key in
        ('W1', 'b1', 'W2', 'b2'):</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;network.params[key]
        -= learning_rate * grad[key]</font></p>
    <p><font face="Trebuchet MS">&nbsp;</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;# 학습 경과 기록</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;loss =
        network.loss(x_batch, t_batch)</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;train_loss_list.append(loss)</font></p>
    <p><font face="Trebuchet MS">&nbsp;</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;# 1에폭당 정확도 계산</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;if i %
        iter_per_epoch == 0:</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;train_acc
        = network.accuracy(x_train, t_train)</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;test_acc
        = network.accuracy(x_test, t_test)</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;train_acc_list.append(train_acc)</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;test_acc_list.append(test_acc)</font></p>
    <p><font face="Trebuchet MS">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("train
        acc, test_acc |" + str(train_acc) + ", " +str(test_acc))</font></p>
  </body>
</html>
